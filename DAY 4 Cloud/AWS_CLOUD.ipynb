{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vw1rzAnWOZy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3\n",
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "u09sXwXjmQx0",
        "outputId": "4befa61a-3ce0-4560-842c-933031df90b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.39.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting botocore<1.40.0,>=1.39.3 (from boto3)\n",
            "  Downloading botocore-1.39.3-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3)\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.3->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.3->boto3) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.40.0,>=1.39.3->boto3) (1.17.0)\n",
            "Downloading boto3-1.39.3-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.39.3-py3-none-any.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.39.3 botocore-1.39.3 jmespath-1.0.1 s3transfer-0.13.0\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Create the data\n",
        "data = {\n",
        "    'ipl_team': ['CSK', 'MI', 'RCB', 'SRH', 'KKR'],\n",
        "    'ipl_wins': [4, 5, 0, 2, 3],\n",
        "    'ipl_team_score': [2850, 2900, 2700, 2750, 2800]\n",
        "}\n",
        "\n",
        "# Step 2: Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 3: Save to CSV\n",
        "filename = 'ipl_teams.csv'\n",
        "df.to_csv(filename, index=False)\n",
        "\n",
        "# Step 4: Download CSV in Google Colab\n",
        "from google.colab import files\n",
        "files.download(filename)"
      ],
      "metadata": {
        "id": "oQS0ZAXPzJkr",
        "outputId": "d80a1b74-7b87-4809-85a9-5caf70fb5696",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_33ca4d53-50cc-4471-a54c-d3720ff6ca21\", \"ipl_teams.csv\", 87)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Python Code Explanation for AWS Lambda\n",
        "\n",
        "This document provides a line-by-line explanation of the provided Python code, formatted for easy viewing and understanding in a Google Colab notebook.\n",
        "\n",
        "### `import json, boto3, pandas as pd, os`\n",
        "\n",
        "```\n",
        "import json, boto3, pandas as pd, os\n",
        "\n",
        "```\n",
        "\n",
        "-   **Explanation:** This line imports the necessary Python libraries.\n",
        "    \n",
        "    -   `json`: For handling JSON (JavaScript Object Notation) data, though not directly used for parsing in this specific snippet, it's often included in Lambda functions.\n",
        "        \n",
        "    -   `boto3`: The Amazon Web Services (AWS) SDK for Python, used to interact with AWS services like S3.\n",
        "        \n",
        "    -   `pandas as pd`: A powerful data manipulation and analysis library, aliased as `pd` for convenience.\n",
        "        \n",
        "    -   `os`: Provides a way of using operating system dependent functionality, specifically for accessing environment variables here.\n",
        "        \n",
        "\n",
        "### `s3_client = boto3.client('s3')`\n",
        "\n",
        "```\n",
        "s3_client = boto3.client('s3')\n",
        "\n",
        "```\n",
        "\n",
        "-   **Explanation:** Initializes a client object for Amazon S3 (Simple Storage Service). This client will be used to perform operations like getting and putting objects in S3 buckets.\n",
        "    \n",
        "\n",
        "### `bucket_name = os.environ.get('S3_BUCKET_NAME')`\n",
        "\n",
        "```\n",
        "bucket_name = os.environ.get('S3_BUCKET_NAME')\n",
        "\n",
        "```\n",
        "\n",
        "-   **Explanation:** Retrieves the name of the S3 bucket from the Lambda function's environment variables. Environment variables are a common way to configure Lambda functions without hardcoding values in the code.\n",
        "    \n",
        "\n",
        "### `input_file_key = 'ipl_data.csv'`\n",
        "\n",
        "```\n",
        "input_file_key = 'ipl_data.csv'\n",
        "\n",
        "```\n",
        "\n",
        "-   **Explanation:** Defines the key (path/name) of the input CSV file within the S3 bucket. This is the file that the Lambda function will download and process.\n",
        "    \n",
        "\n",
        "### `output_file_key = 'processed_ipl_data.csv'`\n",
        "\n",
        "```\n",
        "output_file_key = 'processed_ipl_data.csv'\n",
        "\n",
        "```\n",
        "\n",
        "-   **Explanation:** Defines the key (path/name) for the output processed CSV file. After processing, the modified data will be uploaded back to S3 with this name.\n",
        "    \n",
        "\n",
        "### `response = s3_client.get_object(Bucket=bucket_name, Key=input_file_key)`\n",
        "\n",
        "```\n",
        "response = s3_client.get_object(Bucket=bucket_name, Key=input_file_key)\n",
        "\n",
        "```\n",
        "\n",
        "-   **Explanation:** This line makes an API call to S3 to retrieve the specified input file (`ipl_data.csv`) from the `bucket_name`. The result of this operation (including the file's content) is stored in the `response` variable.\n",
        "    \n",
        "\n",
        "### `csv_content = response['Body'].read().decode('utf-8')`\n",
        "\n",
        "```\n",
        "csv_content = response['Body'].read().decode('utf-8')\n",
        "\n",
        "```\n",
        "\n",
        "-   **Explanation:**\n",
        "    \n",
        "    -   `response['Body']`: Accesses the `Body` stream from the S3 `get_object` response, which contains the file's content.\n",
        "        \n",
        "    -   `.read()`: Reads the entire content of the stream into bytes.\n",
        "        \n",
        "    -   `.decode('utf-8')`: Decodes these bytes into a UTF-8 encoded string, which is necessary for `pandas` to read it as text.\n",
        "        \n",
        "\n",
        "### `df = pd.read_csv(pd.io.common.StringIO(csv_content))`\n",
        "\n",
        "```\n",
        "df = pd.read_csv(pd.io.common.StringIO(csv_content))\n",
        "\n",
        "```\n",
        "\n",
        "-   **Explanation:**\n",
        "    \n",
        "    -   `pd.io.common.StringIO(csv_content)`: This is a crucial step. `StringIO` is a module that allows you to treat a string as if it were a file. Pandas' `read_csv` function typically expects a file path or a file-like object. By wrapping `csv_content` (which is a string) in `StringIO`, we make it appear as a file to `read_csv`.\n",
        "        \n",
        "    -   `pd.read_csv(...)`: Reads the CSV data from the `StringIO` object directly into a pandas DataFrame, named `df`.\n",
        "        \n",
        "\n",
        "### `df['can_win'] = ((df['ipl_wins'] >= 3) & (df['ipl_team_score'] >= 2800)).astype(int)`\n",
        "\n",
        "```\n",
        "df['can_win'] = ((df['ipl_wins'] >= 3) & (df['ipl_team_score'] >= 2800)).astype(int)\n",
        "\n",
        "```\n",
        "\n",
        "-   **Explanation:** This is the core data preprocessing logic.\n",
        "    \n",
        "    -   `df['ipl_wins'] >= 3`: Creates a boolean Series where `True` indicates teams with 3 or more IPL wins.\n",
        "        \n",
        "    -   `df['ipl_team_score'] >= 2800`: Creates another boolean Series where `True` indicates teams with an IPL team score of 2800 or more.\n",
        "        \n",
        "    -   `&`: This is the bitwise AND operator, used here for element-wise logical AND between the two boolean Series. A team `can_win` only if _both_ conditions are `True`.\n",
        "        \n",
        "    -   `.astype(int)`: Converts the resulting boolean Series (`True`/`False`) into integers (`1`/`0`). `1` represents `True` (the team can win), and `0` represents `False` (the team cannot win).\n",
        "        \n",
        "    -   `df['can_win'] = ...`: Assigns this new Series as a new column named `can_win` to the DataFrame `df`.\n",
        "        \n",
        "\n",
        "### `processed_csv_content = df.to_csv(index=False)`\n",
        "\n",
        "```\n",
        "processed_csv_content = df.to_csv(index=False)\n",
        "\n",
        "```\n",
        "\n",
        "-   **Explanation:**\n",
        "    \n",
        "    -   `df.to_csv()`: Converts the modified DataFrame `df` back into a CSV formatted string.\n",
        "        \n",
        "    -   `index=False`: This argument is important. By default, `to_csv` writes the DataFrame's index as the first column in the CSV. Setting `index=False` prevents this, resulting in a cleaner CSV output.\n",
        "        \n",
        "\n",
        "### `s3_client.put_object(Bucket=bucket_name, Key=output_file_key, Body=processed_csv_content)`\n",
        "\n",
        "```\n",
        "s3_client.put_object(Bucket=bucket_name, Key=output_file_key, Body=processed_csv_content)\n",
        "\n",
        "```\n",
        "\n",
        "-   **Explanation:** This line uploads the `processed_csv_content` (the string containing the processed CSV data) back to the specified S3 `bucket_name` with the `output_file_key` (`processed_ipl_data.csv`).\n",
        "    \n",
        "\n",
        "### `return {'statusCode': 200, ...}`\n",
        "\n",
        "```\n",
        "return {'statusCode': 200, 'body': json.dumps('CSV processed and uploaded successfully!')}\n",
        "\n",
        "```\n",
        "\n",
        "-   **Explanation:** This is the standard return format for an AWS Lambda function.\n",
        "    \n",
        "    -   `'statusCode': 200`: Indicates a successful execution (HTTP 200 OK).\n",
        "        \n",
        "    -   `'body': json.dumps(...)`: The response body, which is typically a JSON string. Here, it's a success message. `json.dumps()` converts a Python dictionary/string into a JSON formatted string.\n",
        "        \n",
        "\n",
        "### `except Exception as e: ...`\n",
        "\n",
        "```\n",
        "except Exception as e:\n",
        "    print(f\"Error processing file: {e}\")\n",
        "    return {'statusCode': 500, 'body': json.dumps(f\"Error: {e}\")}\n",
        "\n",
        "```\n",
        "\n",
        "-   **Explanation:** This is a basic error handling block.\n",
        "    \n",
        "    -   `except Exception as e:`: Catches any general exception that might occur during the execution of the `try` block.\n",
        "        \n",
        "    -   `print(f\"Error processing file: {e}\")`: Prints the error message to the Lambda logs (CloudWatch Logs), which is crucial for debugging.\n",
        "        \n",
        "    -   `return {'statusCode': 500, ...}`: Returns an error response to the caller, indicating a server error (HTTP 500 Internal Server Error) along with the specific error message."
      ],
      "metadata": {
        "id": "d-FSPjgJmniY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j9-us4Rltxl"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Initialize S3 client\n",
        "s3_client = boto3.client('s3')\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    # Line 1: Get the S3 bucket name from environment variables\n",
        "    bucket_name = os.environ.get('S3_BUCKET_NAME')\n",
        "    # Line 2: Define the input file key (path) in S3\n",
        "    input_file_key = 'ipl_data.csv'\n",
        "    # Line 3: Define the output file key (path) in S3 for processed data\n",
        "    output_file_key = 'processed_ipl_data.csv'\n",
        "\n",
        "    print(f\"Reading {input_file_key} from bucket {bucket_name}\")\n",
        "\n",
        "    try:\n",
        "        # Line 4: Download the CSV file from S3\n",
        "        response = s3_client.get_object(Bucket=bucket_name, Key=input_file_key)\n",
        "        # Line 5: Read the content of the file\n",
        "        csv_content = response['Body'].read().decode('utf-8')\n",
        "        # Line 6: Use pandas to read the CSV content into a DataFrame\n",
        "        df = pd.read_csv(pd.io.common.StringIO(csv_content))\n",
        "\n",
        "        print(\"Original DataFrame:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Line 7: Simple preprocessing: Create a 'target' column (binary classifier)\n",
        "        # Predict if a team can win based on ipl_wins and ipl_team_score\n",
        "        # For simplicity, let's say a team \"can win\" if ipl_wins >= 3 AND ipl_team_score >= 2800\n",
        "        df['can_win'] = ((df['ipl_wins'] >= 3) & (df['ipl_team_score'] >= 2800)).astype(int)\n",
        "\n",
        "        print(\"Processed DataFrame:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Line 8: Convert the processed DataFrame back to CSV format (without index)\n",
        "        processed_csv_content = df.to_csv(index=False)\n",
        "\n",
        "        # Line 9: Upload the processed CSV back to S3\n",
        "        s3_client.put_object(Bucket=bucket_name, Key=output_file_key, Body=processed_csv_content)\n",
        "\n",
        "        print(f\"Successfully processed {input_file_key} and saved to {output_file_key} in {bucket_name}\")\n",
        "\n",
        "        return {\n",
        "            'statusCode': 200,\n",
        "            'body': json.dumps(f'Successfully processed data and stored in s3://{bucket_name}/{output_file_key}')\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data: {e}\")\n",
        "        return {\n",
        "            'statusCode': 500,\n",
        "            'body': json.dumps(f'Error processing data: {str(e)}')\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Code Explanation (No Pandas) for AWS Lambda\n",
        "\n",
        "This document provides a line-by-line explanation of the provided Python code, which processes CSV data without relying on the Pandas library, formatted for easy viewing and understanding in a Google Colab notebook.\n",
        "\n",
        "### `import csv, StringIO`\n",
        "\n",
        "import csv, StringIO\n",
        "\n",
        "-   **Explanation:** This line imports the necessary Python modules.\n",
        "    \n",
        "    -   `csv`: This module provides classes and functions to facilitate the reading and writing of tabular data in CSV format.\n",
        "        \n",
        "    -   `StringIO`: This module allows you to treat a string as an in-memory text file. This is crucial for `csv.reader` to process string content as if it were reading from a physical file.\n",
        "        \n",
        "\n",
        "### `csv_file = StringIO(csv_content)`\n",
        "\n",
        "csv_file = StringIO(csv_content)\n",
        "\n",
        "-   **Explanation:** This line takes the raw CSV content (assumed to be a string variable named `csv_content`, likely obtained from an S3 object) and wraps it within a `StringIO` object. This makes the string behave like a file, which is a prerequisite for the `csv.reader` to be able to parse it.\n",
        "    \n",
        "\n",
        "### `reader = csv.reader(csv_file)`\n",
        "\n",
        "reader = csv.reader(csv_file)\n",
        "\n",
        "-   **Explanation:** This creates a `reader` object from the `csv` module. This `reader` object is an iterator that will iterate over lines in the `csv_file` (our `StringIO` object) and parse them into lists of strings, handling CSV specific formatting like delimiters and quotes.\n",
        "    \n",
        "\n",
        "### `header = next(reader)`\n",
        "\n",
        "header = next(reader)\n",
        "\n",
        "-   **Explanation:** The `next()` function is used to retrieve the first item from an iterator. In this context, it reads the very first row of the CSV file, which is typically the header row containing column names. This row is then stored in the `header` list.\n",
        "    \n",
        "\n",
        "### `data = list(reader)`\n",
        "\n",
        "data = list(reader)\n",
        "\n",
        "-   **Explanation:** After the `header` has been read, the `reader` iterator is positioned at the second row. Calling `list(reader)` consumes the rest of the iterator, reading all remaining rows from the CSV file. Each row is parsed into a list of strings, and all these row lists are collected into a single list called `data`.\n",
        "    \n",
        "\n",
        "### `header.append('can_win')`\n",
        "\n",
        "header.append('can_win')\n",
        "\n",
        "-   **Explanation:** This line modifies the `header` list by adding a new string, `'can_win'`, to its end. This prepares the header for the new column that will be generated based on the processing logic.\n",
        "    \n",
        "\n",
        "### `for row in data:`\n",
        "\n",
        "for row in data:\n",
        "\n",
        "-   **Explanation:** This initiates a loop that iterates through each `row` in the `data` list. Each `row` inside this loop will be a list of strings representing a single record from the original CSV file (excluding the header).\n",
        "    \n",
        "\n",
        "### `ipl_wins = int(row[1]), ipl_team_score = int(row[2])`\n",
        "\n",
        "ipl_wins = int(row[1]) ipl_team_score = int(row[2])\n",
        "\n",
        "-   **Explanation:** Inside the loop, these lines extract specific values from the current `row`.\n",
        "    \n",
        "    -   `row[1]`: Accesses the element at index 1 (the second column) of the current row, which is assumed to be the 'ipl_wins' value.\n",
        "        \n",
        "    -   `row[2]`: Accesses the element at index 2 (the third column) of the current row, assumed to be the 'ipl_team_score' value.\n",
        "        \n",
        "    -   `int(...)`: Converts these string values into integers, as numerical comparisons are needed for the classification logic.\n",
        "        \n",
        "\n",
        "### `can_win = 1 if (ipl_wins >= 3 and ipl_team_score >= 2800) else 0`\n",
        "\n",
        "can_win = 1 if (ipl_wins >= 3 and ipl_team_score >= 2800) else 0\n",
        "\n",
        "-   **Explanation:** This is the core classification logic, implemented using a conditional expression (ternary operator).\n",
        "    \n",
        "    -   `ipl_wins >= 3 and ipl_team_score >= 2800`: This checks if both conditions are true: the team has 3 or more IPL wins AND an IPL team score of 2800 or more.\n",
        "        \n",
        "    -   `1 if ... else 0`: If both conditions are `True`, `can_win` is assigned the integer `1`. Otherwise, it's assigned `0`.\n",
        "        \n",
        "\n",
        "### `new_row = row + [str(can_win)]`\n",
        "\n",
        "new_row = row + [str(can_win)]\n",
        "\n",
        "-   **Explanation:** This line constructs a `new_row` for the processed data.\n",
        "    \n",
        "    -   `row`: Takes the original list of string values for the current row.\n",
        "        \n",
        "    -   `[str(can_win)]`: Creates a new list containing only the `can_win` value, which is converted back to a string because all elements in a CSV row are typically strings.\n",
        "        \n",
        "    -   `+`: Concatenates the original `row` list with the new list containing `can_win`, effectively appending the `can_win` value as a new column to the row.\n",
        "        \n",
        "\n",
        "### `processed_csv_content = output_csv_file.getvalue()`\n",
        "\n",
        "processed_csv_content = output_csv_file.getvalue()\n",
        "\n",
        "-   **Explanation:** (Assuming `output_csv_file` is a `StringIO` object where processed rows are being written by a `csv.writer`) This line retrieves the complete string content that has been written to the `output_csv_file` (the `StringIO` object). This string will contain the header followed by all the processed data rows, formatted as a CSV.\n",
        "    \n",
        "\n",
        "### `The rest is similar to the Pandas version for S3 interaction and error handling.`\n",
        "\n",
        "The rest is similar to the Pandas version for S3 interaction and error handling.\n",
        "\n",
        "-   **Explanation:** This statement indicates that the subsequent parts of the Lambda function (uploading the `processed_csv_content` back to S3 using `s3_client.put_object` and the `try-except` block for error handling) would be identical in structure and function to the Pandas version previously explained.\n",
        "    \n",
        "\n",
        "### Common AWS Error: \"Invalid Lambda function policy.\"\n",
        "\n",
        "Common AWS Error: \"Invalid Lambda function policy.\" This might happen if you are trying to attach an invalid policy or have a typo. Review the policy syntax.\n",
        "\n",
        "-   **Explanation:** This is a common error encountered when deploying AWS Lambda functions. It suggests that the IAM (Identity and Access Management) policy attached to your Lambda function's execution role is malformed, contains incorrect syntax, or grants permissions in an invalid way. To resolve this, you would need to carefully review the JSON policy document for syntax errors, ensure all actions and resources are correctly specified, and verify that the policy adheres to IAM policy best practices."
      ],
      "metadata": {
        "id": "nM8Hs3DUpvF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import boto3\n",
        "import os\n",
        "import csv\n",
        "from io import StringIO\n",
        "\n",
        "s3_client = boto3.client('s3')\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    bucket_name = os.environ.get('S3_BUCKET_NAME')\n",
        "    input_file_key = 'ipl_data.csv'\n",
        "    output_file_key = 'processed_ipl_data.csv'\n",
        "\n",
        "    print(f\"Reading {input_file_key} from bucket {bucket_name}\")\n",
        "\n",
        "    try:\n",
        "        response = s3_client.get_object(Bucket=bucket_name, Key=input_file_key)\n",
        "        csv_content = response['Body'].read().decode('utf-8')\n",
        "\n",
        "        # Use StringIO to treat the string as a file for csv.reader\n",
        "        csv_file = StringIO(csv_content)\n",
        "        reader = csv.reader(csv_file)\n",
        "        header = next(reader) # Read header row\n",
        "        data = list(reader)  # Read remaining data rows\n",
        "\n",
        "        # Add 'can_win' to header\n",
        "        header.append('can_win')\n",
        "\n",
        "        processed_rows = []\n",
        "        for row in data:\n",
        "            ipl_wins = int(row[1]) # ipl_wins is the second column (index 1)\n",
        "            ipl_team_score = int(row[2]) # ipl_team_score is the third column (index 2)\n",
        "\n",
        "            # Simple preprocessing logic\n",
        "            can_win = 1 if (ipl_wins >= 3 and ipl_team_score >= 2800) else 0\n",
        "            new_row = row + [str(can_win)] # Append the new value\n",
        "            processed_rows.append(new_row)\n",
        "\n",
        "        # Prepare content for writing back to S3\n",
        "        output_csv_file = StringIO()\n",
        "        writer = csv.writer(output_csv_file)\n",
        "        writer.writerow(header) # Write header\n",
        "        writer.writerows(processed_rows) # Write processed rows\n",
        "        processed_csv_content = output_csv_file.getvalue()\n",
        "\n",
        "        s3_client.put_object(Bucket=bucket_name, Key=output_file_key, Body=processed_csv_content)\n",
        "\n",
        "        print(f\"Successfully processed {input_file_key} and saved to {output_file_key} in {bucket_name}\")\n",
        "\n",
        "        return {\n",
        "            'statusCode': 200,\n",
        "            'body': json.dumps(f'Successfully processed data and stored in s3://{bucket_name}/{output_file_key}')\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data: {e}\")\n",
        "        return {\n",
        "            'statusCode': 500,\n",
        "            'body': json.dumps(f'Error processing data: {str(e)}')\n",
        "        }"
      ],
      "metadata": {
        "id": "REDsxPR7oe5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Line 1: Install scikit-learn if it's not already available in the environment\n",
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "\n",
        "import pandas as pd\n",
        "import boto3\n",
        "import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib # For saving/loading models\n",
        "\n",
        "# Line 1: Define your S3 bucket name\n",
        "bucket_name = 'ipl-mlops-workshop-yourname-date' # REPLACE with your S3 bucket name\n",
        "# Line 2: Define the key for the processed data file\n",
        "processed_data_key = 'processed_ipl_data.csv'\n",
        "# Line 3: Define the key for saving the trained model\n",
        "model_key = 'ipl_winner_predictor.joblib'\n",
        "\n",
        "# Line 4: Initialize S3 client\n",
        "s3 = boto3.client('s3')\n",
        "\n",
        "print(f\"Downloading {processed_data_key} from {bucket_name}...\")\n",
        "# Line 5: Get the object from S3\n",
        "obj = s3.get_object(Bucket=bucket_name, Key=processed_data_key)\n",
        "# Line 6: Read the object body and decode it\n",
        "body = obj['Body'].read().decode('utf-8')\n",
        "# Line 7: Use io.StringIO to read the string content as a CSV file\n",
        "df = pd.read_csv(io.StringIO(body))\n",
        "\n",
        "print(\"Data loaded successfully:\")\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "\n",
        "# Line 1: Define features (X) and target (y)\n",
        "X = df[['ipl_wins', 'ipl_team_score']] # Features for prediction\n",
        "y = df['can_win'] # Target variable\n",
        "\n",
        "print(\"Features (X) head:\")\n",
        "print(X.head())\n",
        "print(\"Target (y) head:\")\n",
        "print(y.head())\n",
        "\n",
        "# Line 2: Split data into training and testing sets (80% train, 20% test)\n",
        "# random_state for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n",
        "\n",
        "# Line 1: Initialize the Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "\n",
        "print(\"Training model...\")\n",
        "# Line 2: Train the model using the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Line 3: Evaluate the model on the test set\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"Model accuracy on test set: {accuracy:.2f}\")\n",
        "\n",
        "# Line 1: Save the trained model locally as a joblib file\n",
        "local_model_path = '/tmp/ipl_winner_predictor.joblib' # SageMaker notebook instances have /tmp for temporary storage\n",
        "joblib.dump(model, local_model_path)\n",
        "\n",
        "print(f\"Model saved locally to {local_model_path}\")\n",
        "\n",
        "# Line 2: Upload the saved model to S3\n",
        "s3.upload_file(local_model_path, bucket_name, model_key)\n",
        "\n",
        "print(f\"Model uploaded to s3://{bucket_name}/{model_key}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "P_6T26iZpuqu",
        "outputId": "ca4947c2-75e3-42cd-d2b4-6469876ed05b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'boto3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2603371207.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'boto3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import boto3\n",
        "import os\n",
        "import joblib\n",
        "import io\n",
        "import pandas as pd # Although we avoided for preprocessing, it's common for inference\n",
        "\n",
        "# Line 1: Initialize S3 client\n",
        "s3_client = boto3.client('s3')\n",
        "\n",
        "# Line 2: Define S3 bucket and model key from environment variables\n",
        "BUCKET_NAME = os.environ.get('S3_BUCKET_NAME')\n",
        "MODEL_KEY = 'ipl_winner_predictor.joblib'\n",
        "LOCAL_MODEL_PATH = '/tmp/ipl_winner_predictor.joblib' # Temporary path in Lambda execution environment\n",
        "\n",
        "# Line 3: Global variable to store the loaded model, to avoid re-loading on subsequent invocations (warm start)\n",
        "model = None\n",
        "\n",
        "def load_model():\n",
        "    global model\n",
        "    if model is None:\n",
        "        print(f\"Loading model from s3://{BUCKET_NAME}/{MODEL_KEY}...\")\n",
        "        try:\n",
        "            # Line 4: Download model from S3\n",
        "            s3_client.download_file(BUCKET_NAME, MODEL_KEY, LOCAL_MODEL_PATH)\n",
        "            # Line 5: Load the model using joblib\n",
        "            model = joblib.load(LOCAL_MODEL_PATH)\n",
        "            print(\"Model loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            raise e\n",
        "    return model\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    # Line 6: Load the model (or use existing loaded model)\n",
        "    current_model = load_model()\n",
        "\n",
        "    # Line 7: Parse the request body (assuming JSON input from API Gateway)\n",
        "    try:\n",
        "        body = json.loads(event['body'])\n",
        "        ipl_wins = body['ipl_wins']\n",
        "        ipl_team_score = body['ipl_team_score']\n",
        "    except KeyError:\n",
        "        return {\n",
        "            'statusCode': 400,\n",
        "            'body': json.dumps('Missing \"ipl_wins\" or \"ipl_team_score\" in request body.')\n",
        "        }\n",
        "    except json.JSONDecodeError:\n",
        "        return {\n",
        "            'statusCode': 400,\n",
        "            'body': json.dumps('Invalid JSON in request body.')\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'statusCode': 500,\n",
        "            'body': json.dumps(f'Error parsing input: {str(e)}')\n",
        "        }\n",
        "\n",
        "    # Line 8: Prepare input for the model (must match training features order)\n",
        "    # Create a DataFrame for consistent input to scikit-learn model\n",
        "    input_data = pd.DataFrame([[ipl_wins, ipl_team_score]], columns=['ipl_wins', 'ipl_team_score'])\n",
        "\n",
        "    # Line 9: Make prediction\n",
        "    try:\n",
        "        prediction = current_model.predict(input_data)[0]\n",
        "        # Line 10: Convert numpy.int64 to standard int for JSON serialization\n",
        "        prediction_result = int(prediction)\n",
        "        print(f\"Prediction for ipl_wins={ipl_wins}, ipl_team_score={ipl_team_score}: {prediction_result}\")\n",
        "\n",
        "        return {\n",
        "            'statusCode': 200,\n",
        "            'headers': {\n",
        "                'Content-Type': 'application/json'\n",
        "            },\n",
        "            'body': json.dumps({\n",
        "                'prediction': prediction_result,\n",
        "                'message': 'Team can win' if prediction_result == 1 else 'Team cannot win'\n",
        "            })\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "        return {\n",
        "            'statusCode': 500,\n",
        "            'body': json.dumps(f'Error during prediction: {str(e)}')\n",
        "        }"
      ],
      "metadata": {
        "id": "anzlJvlKtTUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 1: Clean and prepare directories\n",
        "!rm -rf python joblib_pandas_sklearn_layer.zip\n",
        "!mkdir -p python\n",
        "\n",
        "# ✅ Step 2: Install required packages into 'python/' (Lambda layer structure)\n",
        "!pip install joblib -t python/\n",
        "\n",
        "# ✅ Step 3: Zip it into a Lambda-compatible layer\n",
        "!zip -r joblib_pandas_sklearn_layer.zip python > /dev/null\n",
        "\n",
        "# ✅ Step 4: Download the zip file\n",
        "from google.colab import files\n",
        "files.download(\"joblib_pandas_sklearn_layer.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "gegvEKwaxce2",
        "outputId": "9b9756cb-8b9c-4dc6-a308-1e9fb9933f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas\n",
            "  Using cached pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "Collecting joblib\n",
            "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting numpy>=1.23.2 (from pandas)\n",
            "  Using cached numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas)\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Using cached pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
            "Using cached numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, tzdata, six, numpy, joblib, python-dateutil, pandas\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.5.1 numpy-2.3.1 pandas-2.3.0 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "numpy",
                  "six"
                ]
              },
              "id": "bd983d44a1e84f5b855a1fbe95bc7428"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2475c3e6-dfb1-40de-be61-3c4868b0bd3a\", \"joblib_pandas_sklearn_layer.zip\", 47639848)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}